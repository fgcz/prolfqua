% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Benchmark.R
\name{Benchmark}
\alias{Benchmark}
\title{Benchmark R6 class}
\description{
Benchmark R6 class

Benchmark R6 class
}
\examples{

dd <- dplyr::filter(prolfqua_data("data_benchmarkExample"), !is.na(statistic))
dd <- dd |> dplyr::mutate(avgInt = (c1 + c2) / 2)
ttd <- ionstar_bench_preprocess(dd)
medpol_benchmark <- make_benchmark(ttd$data,
  benchmark = list(
    list(score = "estimate", desc = TRUE),
    list(score = "statistic", desc = TRUE),
    list(score = "scaled.p.value", desc = TRUE)
  ),
  fcestimate = "estimate",
  model_description = "med. polish and lm. density",
  model_name = "prot_med_lm"
)
medpol_benchmark$plot_score_distribution(list(
  list(score = "estimate", xlim = c(-1, 2)),
  list(score = "statistic", xlim = c(-3, 10))
))

# Benchmark$debug("plot_score_distribution")
benchmark <- make_benchmark(
  ttd$data,
  toscale = c("moderated.p.value", "moderated.p.value.adjusted"),
  fcestimate = "estimate",
  benchmark = list(
    list(score = "estimate", desc = TRUE),
    list(score = "statistic", desc = TRUE),
    list(score = "scaled.moderated.p.value", desc = TRUE),
    list(score = "scaled.moderated.p.value.adjusted", desc = TRUE)
  ),
  FDRvsFDP =
    list(
      list(score = "moderated.p.value", desc = FALSE),
      list(score = "moderated.p.value.adjusted", desc = FALSE)
    ),
  model_description = "protein level measurments, lm model",
  model_name = "prot_lm"
)
bb <- benchmark$pAUC_summaries()
benchmark$complete(FALSE)
benchmark$smc$summary
benchmark$plot_score_distribution(list(list(score = "estimate", xlim = c(-1, 2)), list(score = "statistic", xlim = c(-3, 10))))
benchmark$plot_score_distribution()

bb <- benchmark$get_confusion_FDRvsFDP()
xb <- dplyr::filter(bb, contrast == "dilution_(4.5/3)_1.5")
bb <- benchmark$get_confusion_benchmark()
benchmark$plot_ROC(xlim = 0.1)
benchmark$plot_FDPvsTPR()
benchmark$plot_FDRvsFDP()
benchmark$plot_scatter(list(list(score = "estimate", ylim = c(-1, 2)), list(score = "statistic", ylim = c(-3, 10))))
benchmark$complete(FALSE)
benchmark$missing_contrasts()
stopifnot(nrow(benchmark$pAUC_summaries()$ftable$content) == 4 * (4 + 1))
benchmark$complete(TRUE)
stopifnot(nrow(benchmark$pAUC_summaries()$ftable$content) == 4 * (4 + 1))
missum <- benchmark$missing_contrasts()$summary
stopifnot(nrow(missum) == 4)
stopifnot(ncol(missum) == 2)
# returns number of statistics
stopifnot(nrow(benchmark$n_confusion_benchmark()) == 4 * (4 + 1))
stopifnot(nrow(benchmark$n_confusion_FDRvsFDP()) == 2 * (4 + 1))
benchmark$pAUC()
}
\seealso{
Other benchmarking: 
\code{\link{INTERNAL_FUNCTIONS_BY_FAMILY}},
\code{\link{ionstar_bench_preprocess}()},
\code{\link{make_benchmark}()},
\code{\link{ms_bench_auc}()}
}
\concept{benchmarking}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{.data}}{data.frame}

\item{\code{is_complete}}{todo}

\item{\code{contrast}}{column name}

\item{\code{toscale}}{which columns to scale}

\item{\code{avgInt}}{average Intensity}

\item{\code{fcestimate}}{estimate column}

\item{\code{benchmark}}{todo}

\item{\code{model_description}}{describe model}

\item{\code{model_name}}{model description}

\item{\code{hierarchy}}{todo}

\item{\code{smc}}{summarize missing contrasts}

\item{\code{summarizeNA}}{statistic to use for missigness summarization (e.g. statistic, or p-value)}

\item{\code{confusion}}{todo}

\item{\code{species}}{todo}

\item{\code{FDRvsFDP}}{todo}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-Benchmark-new}{\code{Benchmark$new()}}
\item \href{#method-Benchmark-data}{\code{Benchmark$data()}}
\item \href{#method-Benchmark-missing_contrasts}{\code{Benchmark$missing_contrasts()}}
\item \href{#method-Benchmark-complete}{\code{Benchmark$complete()}}
\item \href{#method-Benchmark-.get_confusion}{\code{Benchmark$.get_confusion()}}
\item \href{#method-Benchmark-get_confusion_benchmark}{\code{Benchmark$get_confusion_benchmark()}}
\item \href{#method-Benchmark-n_confusion_benchmark}{\code{Benchmark$n_confusion_benchmark()}}
\item \href{#method-Benchmark-plot_FDPvsTPR}{\code{Benchmark$plot_FDPvsTPR()}}
\item \href{#method-Benchmark-plot_ROC}{\code{Benchmark$plot_ROC()}}
\item \href{#method-Benchmark-pAUC_summaries}{\code{Benchmark$pAUC_summaries()}}
\item \href{#method-Benchmark-pAUC}{\code{Benchmark$pAUC()}}
\item \href{#method-Benchmark-get_confusion_FDRvsFDP}{\code{Benchmark$get_confusion_FDRvsFDP()}}
\item \href{#method-Benchmark-n_confusion_FDRvsFDP}{\code{Benchmark$n_confusion_FDRvsFDP()}}
\item \href{#method-Benchmark-plot_FDRvsFDP}{\code{Benchmark$plot_FDRvsFDP()}}
\item \href{#method-Benchmark-plot_score_distribution}{\code{Benchmark$plot_score_distribution()}}
\item \href{#method-Benchmark-plot_scatter}{\code{Benchmark$plot_scatter()}}
\item \href{#method-Benchmark-clone}{\code{Benchmark$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-new"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-new}{}}}
\subsection{Method \code{new()}}{
create Benchmark
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$new(
  data,
  toscale = c("p.value"),
  fcestimate = "diff",
  avgInt = "avgInt",
  benchmark = list(list(score = "diff", desc = TRUE), list(score = "statistic", desc =
    TRUE), list(score = "scaled.p.value", desc = TRUE)),
  FDRvsFDP = list(list(score = "FDR", desc = FALSE)),
  model_description = "protein level measurments, linear model",
  model_name = "medpolish_lm",
  contrast = "contrast",
  species = "species",
  hierarchy = c("protein_Id"),
  summarizeNA = "statistic"
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{data.frame}

\item{\code{toscale}}{columns ot scale}

\item{\code{fcestimate}}{column with fold change estimates}

\item{\code{avgInt}}{average protein/peptide/metabolite intensity}

\item{\code{benchmark}}{columns to benchmark}

\item{\code{FDRvsFDP}}{score for which to generate FDR vs FDP}

\item{\code{model_description}}{describe model}

\item{\code{model_name}}{model name}

\item{\code{contrast}}{contrast}

\item{\code{species}}{species (todo rename)}

\item{\code{hierarchy}}{e.g. protein_Id}

\item{\code{summarizeNA}}{examine this column to determine the proportion of missing values default statistic}

\item{\code{columns}}{to create FPR vs FDP analysis for}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-data"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-data}{}}}
\subsection{Method \code{data()}}{
get data
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$data()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
data.frame
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-missing_contrasts"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-missing_contrasts}{}}}
\subsection{Method \code{missing_contrasts()}}{
summarize missing contrasts
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$missing_contrasts()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
data.frame
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-complete"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-complete}{}}}
\subsection{Method \code{complete()}}{
set or get complete.
If true only proteins for which all contrasts are determinable are examined.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$complete(value)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{value}}{TRUE if data should be complete (no missing contrasts)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-.get_confusion"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-.get_confusion}{}}}
\subsection{Method \code{.get_confusion()}}{
get confusion data
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$.get_confusion(arrange)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{arrange}}{todo}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-get_confusion_benchmark"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-get_confusion_benchmark}{}}}
\subsection{Method \code{get_confusion_benchmark()}}{
get FDR summaries
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$get_confusion_benchmark()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-n_confusion_benchmark"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-n_confusion_benchmark}{}}}
\subsection{Method \code{n_confusion_benchmark()}}{
nr of elements used to determine ROC curve
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$n_confusion_benchmark()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-plot_FDPvsTPR"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-plot_FDPvsTPR}{}}}
\subsection{Method \code{plot_FDPvsTPR()}}{
plot FDP vs TPR
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$plot_FDPvsTPR(xlim = 0.5)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{xlim}}{limit x axis}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-plot_ROC"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-plot_ROC}{}}}
\subsection{Method \code{plot_ROC()}}{
plot FDR summaries
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$plot_ROC(xlim = 0.5)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{xlim}}{limit x axis}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
ggplot
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-pAUC_summaries"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-pAUC_summaries}{}}}
\subsection{Method \code{pAUC_summaries()}}{
AUC summaries
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$pAUC_summaries()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-pAUC"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-pAUC}{}}}
\subsection{Method \code{pAUC()}}{
AUC summaries as table
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$pAUC()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-get_confusion_FDRvsFDP"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-get_confusion_FDRvsFDP}{}}}
\subsection{Method \code{get_confusion_FDRvsFDP()}}{
FDR vs FDP data
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$get_confusion_FDRvsFDP()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-n_confusion_FDRvsFDP"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-n_confusion_FDRvsFDP}{}}}
\subsection{Method \code{n_confusion_FDRvsFDP()}}{
nr of elements used to determine ROC curve
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$n_confusion_FDRvsFDP()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-plot_FDRvsFDP"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-plot_FDRvsFDP}{}}}
\subsection{Method \code{plot_FDRvsFDP()}}{
plot FDR vs FDP data
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$plot_FDRvsFDP()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
ggplot
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-plot_score_distribution"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-plot_score_distribution}{}}}
\subsection{Method \code{plot_score_distribution()}}{
plot distributions of scores
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$plot_score_distribution(score)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{score}}{the distribution of which scores to plot (list)}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
ggplot
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-plot_scatter"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-plot_scatter}{}}}
\subsection{Method \code{plot_scatter()}}{
plot intensity vs scores
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$plot_scatter(score)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{score}}{the distribution of which scores to plot (list)}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
ggplot
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-Benchmark-clone"></a>}}
\if{latex}{\out{\hypertarget{method-Benchmark-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Benchmark$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
